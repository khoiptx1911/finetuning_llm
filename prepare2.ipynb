{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'train': Dataset({\n",
      "    features: ['input_ids', 'attention_mask'],\n",
      "    num_rows: 605\n",
      "}), 'test': Dataset({\n",
      "    features: ['input_ids', 'attention_mask'],\n",
      "    num_rows: 152\n",
      "})}\n"
     ]
    }
   ],
   "source": [
    "from datasets import load_from_disk, Dataset\n",
    "from transformers import AutoTokenizer\n",
    "\n",
    "# Tải dataset\n",
    "dataset = load_from_disk('./dataset2')\n",
    "\n",
    "# Load tokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"./smollm2_model_1\")\n",
    "\n",
    "# Thêm padding token\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "tokenizer.pad_token_id = tokenizer.eos_token_id\n",
    "\n",
    "def prepare_dataset(dataset):\n",
    "    # Chọn cột QA\n",
    "    train_data = dataset['train']['QA']\n",
    "    test_data = dataset['test']['QA']\n",
    "    \n",
    "    return train_data, test_data\n",
    "\n",
    "# Tách dataset\n",
    "train_data, test_data = prepare_dataset(dataset)\n",
    "\n",
    "# Áp dụng tokenize \n",
    "tokenized_train = tokenizer(train_data, padding='max_length', max_length=450)\n",
    "tokenized_test = tokenizer(test_data, padding='max_length', max_length=450)\n",
    "\n",
    "# Tạo dataset mới từ dữ liệu đã tokenize\n",
    "tokenized_dataset = {\n",
    "    'train': Dataset.from_dict({\n",
    "        'input_ids': tokenized_train['input_ids'],\n",
    "        'attention_mask': tokenized_train['attention_mask']\n",
    "    }),\n",
    "    'test': Dataset.from_dict({\n",
    "        'input_ids': tokenized_test['input_ids'],\n",
    "        'attention_mask': tokenized_test['attention_mask']\n",
    "    })\n",
    "}\n",
    "\n",
    "print(tokenized_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Saving the dataset (1/1 shards): 100%|██████████| 605/605 [00:00<00:00, 32524.40 examples/s]\n",
      "Saving the dataset (1/1 shards): 100%|██████████| 152/152 [00:00<00:00, 12017.61 examples/s]\n"
     ]
    }
   ],
   "source": [
    "# Lưu dataset đã tokenize\n",
    "from datasets import DatasetDict\n",
    "tokenized_dataset_dict = DatasetDict(tokenized_dataset)\n",
    "tokenized_dataset_dict.save_to_disk('./tokenized2')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py1",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
